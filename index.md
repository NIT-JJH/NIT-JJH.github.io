### Welcome to visit Wujie Zhou's Homepage!
<table>
  <tr>
   <td width="70%">
    <h1> Wujie Zhou </h1>
    <p><b> Ph.D., Associate Professor </b></p>
    <p><b> Email: wujiezhou@163.com </b></p> 
    <p> <b><a href="https://www.scholat.com/zhouwujie">中文简历</a> </b> </p>
    <p> <b> Research Interests: </b> </p>
    <p> Computer vision </p>  
    <p> Deep Learning</p>  
    <p> Image Processing</p>      
   </td>
    <td width="30%">
      <img src="/wujiezhou.jpg" width="100%">
   </td>
  </tr>  
</table> 

### Recent News

<div style="text-align:justify;text-justify:inter-ideograph">10/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Instrumentation and Measurement</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">10/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Circuits and Systems for Video Technology</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">10/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Multimedia</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">10/2023 One paper has been accepted by <span style="color:black"><b><i>Expert Systems With Applications</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">10/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Intelligent Transportation Systems</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">09/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Intelligent Vehicles</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">09/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Automation Science and Engineering</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">08/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Geoscience and Remote Sensing</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">08/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Intelligent Transportation Systems</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">07/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Systems, Man, and Cybernetics: Systems</i> </b> </span> </div>

<div style="text-align:justify;text-justify:inter-ideograph">07/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Emerging Topics in Computational Intelligence</i> </b> </span> </div>

<div style="text-align:justify;text-justify:inter-ideograph">05/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Circuits and Systems for Video Technology</i> </b> </span> </div>

<div style="text-align:justify;text-justify:inter-ideograph">05/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Image Processing</i> </b> </span> </div>

<div style="text-align:justify;text-justify:inter-ideograph">02/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</i> </b> </span> </div>

<div style="text-align:justify;text-justify:inter-ideograph">02/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Intelligent Transportation Systems</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">02/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Image Processing</i> </b> </span> </div>

<div style="text-align:justify;text-justify:inter-ideograph">01/2023 One paper has been accepted by <span style="color:black"><b><i>Information Fusion </i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">01/2023 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Instrumentation and Measurement</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">08/2022 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Intelligent Transportation Systems</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">05/2022 One paper has been newly selected as <span style="color:black"><b><i>ESI Hot Paper</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">05/2022 One paper has been accepted by <span style="color:black"><b><i>IEEE Journal of Selected Topics in Signal Processing</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">05/2022 One paper is recognized as <span style="color:black"><b><i> <a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=76" target="_blank"> Popular Documents (the 50 most frequently accessed documents) </a> </i> </b> </span> in <span style="color:black"><b><i> IEEE Transactions on Circuits and Systems for Video Technology.</i> </b> </span> </div> 

<div style="text-align:justify;text-justify:inter-ideograph">04/2022 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Intelligent Vehicles</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">03/2022 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Multimedia</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">03/2022 One paper has been accepted by <span style="color:black"><b><i>IEEE Journal of Selected Topics in Signal Processing</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">03/2022 One paper has been accepted by <span style="color:black"><b><i>IEEE TETCI</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">01/2022 One paper has been newly selected as <span style="color:black"><b><i>ESI Highly Cited Paper</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">01/2022 One paper is recognized as <span style="color:black"><b><i> <a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=83" target="_blank"> Popular Documents (the 50 most frequently accessed documents) </a> </i> </b> </span> in <span style="color:black"><b><i> IEEE Transactions on Image Processing.</i> </b> </span> </div> 

<div style="text-align:justify;text-justify:inter-ideograph">01/2022 One paper is recognized as <span style="color:black"><b><i> <a href="https://ieeexplore.ieee.org/xpl/topAccessedArticles.jsp?punumber=9670" target="_blank"> Popular Documents (the 50 most frequently accessed documents) </a> </i> </b> </span> in <span style="color:black"><b><i> IEEE Intelligent Systems.</i> </b> </span> </div> 

<div style="text-align:justify;text-justify:inter-ideograph">12/2021 One paper has been accepted by <span style="color:black"><b><i>AAAI2022, Acceptance rate 15%</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">11/2021 One paper has been newly selected as <span style="color:black"><b><i>ESI Highly Cited Paper</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">09/2021 One paper has been accepted by <span style="color:black"><b><i>IEEE TETCI</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">09/2021 One paper has been newly selected as <span style="color:black"><b><i>ESI Highly Cited Paper</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">08/2021 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Geoscience and Remote Sensing</i> </b> </span></div>

<div style="text-align:justify;text-justify:inter-ideograph">08/2021 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Image Processing</i> </b> </span> </div>

<div style="text-align:justify;text-justify:inter-ideograph">08/2021 One paper has been accepted by <span style="color:black"><b><i>SCIENCE CHINA Information Sciences （中国科学）</i> </b> </span> </div>

<div style="text-align:justify;text-justify:inter-ideograph">08/2021 One paper has been accepted by <span style="color:black"><b><i>IEEE Transactions on Neural Networks and Learning Systems</i> </b> </span> </div>

### Selected Publications
#### Saliency Prediction / Salient Object Detection

<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">
  
<li style="margin-bottom: 15px"><b>W. Zhou*</b>, Y. Zhu, J. Lei, R. Yang, and L. Yu, "LSNet: Lightweight Spatial Boosting Network for Detecting Salient Objects in RGB-Thermal Images," <b><i>IEEE Transactions on Image Processing </i></b>, doi: 10.1109/TIP.2023.3242775. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/10042233" target="_blank">[IEEE Xplore] </a> <a href="https://github.com/zyrant/LSNet">[Code & Results]</a> 
<br></li>	

<li style="margin-bottom: 15px"><b>W. Zhou*</b>, F. Sun, Q. Jiang, R. Cong, and J.-N. Hwang, "WaveNet: Wavelet Network with Knowledge Distillation for RGB-T Salient Object Detection," <b><i>IEEE Transactions on Image Processing </i></b>, doi: 10.1109/TIP.2023.3275538. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/10127616" target="_blank">[IEEE Xplore] </a> <a href="https://github.com/nowander/WaveNet">[Code & Results]</a> 
<br></li>	
	
<li style="margin-bottom: 15px"><b>W. Zhou*</b>, Q. Guo, J. Lei, L. Yu and J.-N. Hwang, "IRFR-Net: Interactive Recursive Feature-reshaping Network for Detecting Salient Objects in RGB-D Images," <b><i>IEEE Transactions on Neural Networks and Learning Systems</i></b>, doi: 10.1109/TNNLS.2021.3105484. <span style="color:blue">(SCI, ESI Hot Paper, ESI Highly Cited Paper)</span> <a href="https://ieeexplore.ieee.org/document/9519891" target="_blank">[IEEE Xplore] </a> <a href="https://pan.baidu.com/s/1GvyrRANw0Tk3Wg-RvodCcQ">[Results]</a> 提取码: dwdn 
<br></li>

<li style="margin-bottom: 15px"><b>W. Zhou*</b>, Y. Lv, J. Lei and L. Yu, “Global and Local-Contrast Guides Content-Aware Fusion for RGB-D Saliency Prediction,” <b><i>IEEE Transactions on Systems, Man, and Cybernetics: Systems</i></b>, vol. 51, no. 6, pp. 3641-3649, 2021. doi: 10.1109/TSMC.2019.2957386. <span style="color:blue">(SCI, ESI Highly Cited Paper)</span> <a href="https://ieeexplore.ieee.org/document/8941002" target="_blank">[IEEE Xplore] </a> <a href="https://pan.baidu.com/s/1ojHfKJRWaYAsrCTAP8WelQ">[Results]</a> 提取码: 3pgj 
<br></li>

<li style="margin-bottom: 15px"><b>W. Zhou*</b>, J. Wu, J. Lei, J.-N. Hwang and L. Yu, “Salient Object Detection in Stereoscopic 3D Images Using a Deep Convolutional Residual Autoencoder,” <b><i>IEEE Transactions on Multimedia</i></b>, doi: 10.1109/TMM.2020.3025166. <span style="color:blue">(SCI, ESI Highly Cited Paper)</span> <a href="https://ieeexplore.ieee.org/document/9201176" target="_blank">[IEEE Xplore] </a> 
<br></li>

<li style="margin-bottom: 15px"><b>W. Zhou*</b>, Q. Guo, J. Lei, L. Yu and J.-N. Hwang, “ECFFNet: Effective and Consistent Feature Fusion Network for RGB-T Salient Object Detection,” <b><i>IEEE Transactions on Circuits and Systems for Video Technology</i></b>, doi: 10.1109/TCSVT.2021.3077058. <span style="color:blue">(SCI, ESI Hot Paper, ESI Highly Cited Paper)</span> <a href="https://ieeexplore.ieee.org/document/9420662" target="_blank">[IEEE Xplore] </a> <a href="https://pan.baidu.com/s/1Cp6RQMwX3GOTdn3PNyQ72A">[Results]</a> 提取码: tx48 
<br></li>

<li style="margin-bottom: 15px"><b>W. Zhou*</b>, Y. Zhu, J. Lei, J. Wan, and L. Yu, "CCAFNet: Crossflow and cross-scale adaptive fusion network for detecting salient objects in RGB-D images," <b><i>IEEE Transactions on Multimedia</i></b>, doi: 10.1109/TMM.2021.3077767. <span style="color:blue">(SCI, ESI Highly Cited Paper)</span> <a href="https://ieeexplore.ieee.org/document/9424966" target="_blank">[IEEE Xplore] </a> <a href="https://github.com/zyrant/CCAFNet">[Code & Results]</a>
<br></li>

<li style="margin-bottom: 15px"><b>W. Zhou*</b>, W. Liu, J. Lei, T. Luo, L. Yu, “Deep binocular fixation prediction using hierarchical multimodal fusion network,” <b><i>IEEE Transactions on Cognitive and Developmental Systems</i></b>, doi: 10.1109/TCDS.2021.3051010. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/9320595" target="_blank">[IEEE Xplore] </a> 
<br></li>

<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, S. Pan, J. Lei, and L. Yu, "TMFNet: Three-Input Multilevel Fusion Network for Detecting Salient Objects in RGB-D Images," <b><i>IEEE Transactions on Emerging Topics in Computational Intelligence</i></b>, doi: 10.1109/TETCI.2021.3097393. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/9512550" target="_blank">[IEEE Xplore] </a> 
<br></li>
  
<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, Y. Zhu, J. Lei, J. Wan, and L. Yu, "APNet: Adversarial-Learning-Assistance and Perceived Importance Fusion Network for All-Day RGB-T Salient Object Detection," <b><i>IEEE Transactions on Emerging Topics in Computational Intelligence</i></b>, doi: 10.1109/TETCI.2021.3118043. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/9583676/" target="_blank">[IEEE Xplore] </a><a href="https://github.com/zyrant/APNet">[Code & Results]</a> 
<br></li>

<li style="margin-bottom: 15px"><b>W. Zhou*</b>, C. Liu, J. Lei, and L. Yu, "Remaking learning: A lightweight network for saliency redetection on RGB-D images," <b><i>SCIENCE CHINA Information Sciences （中国科学）</i></b>, Accept. https://doi.org/10.1007/s11432-020-3337-9 <span style="color:blue">(SCI)</span> <a href="https://pan.baidu.com/s/1qIPTSdCtbThjASKE8qHA5w">[Results]</a> 提取码: zust 
 <br></li>

</div>
</ul>	

#### RGB–thermal Semantic Segmentation

<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">
	
<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, S. Dong, C. Xu, Y. Qian, “Edge-aware guidance fusion network for RGB–thermal scene parsing,” <b><i>in Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI)</i></b>, Accept, <span style="color:blue"><a href="https://github.com/ShaohuaDong2021/EGFNet">[Code & Results]</a>     (CCF A类) </span>  
<br></li>
	
<li style="margin-bottom: 15px"><b>W. Zhou*</b>, J. Liu, J. Lei, L. Yu and J.-N. Hwang,“GMNet: Graded-Feature Multilabel-Learning Network for RGB-Thermal Urban Scene Semantic Segmentation," <b><i>IEEE Transactions on Image Processing</i></b>, vol. 30, pp. 7790-7802, 2021. doi: 10.1109/TIP.2021.3109518. <span style="color:blue">(SCI, CCF A类, Popular Documents, ESI Hot Paper, ESI Highly Cited Paper)</span> <a href="https://ieeexplore.ieee.org/document/9531449" target="_blank">[IEEE Xplore] </a> <a href="https://github.com/Jinfu0913/GMNet">[Code & Results]</a> 
<br></li>

<li style="margin-bottom: 15px"><b>W. Zhou*</b>, H. Zhang, W. Yan, and W. Lin, “MMSMCNet: Modal Memory Sharing and Morphological Complementary Networks for RGB-T Urban Scene Semantic Segmentation,” <b><i>IEEE Transactions on Circuits and Systems for Video Technology</i></b>, doi: 10.1109/TCSVT.2023.3275314. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/10123009" target="_blank">[IEEE Xplore] </a> <a href="https://github.com/2021nihao/MMSMCNet">[Results]</a>
<br></li>
	
<li style="margin-bottom: 15px"><b>W. Zhou*</b>, X. Lin, J. Lei, L. Yu and J.-N. Hwang, "MFFENet: Multiscale Feature Fusion and Enhancement Network for RGB–Thermal Urban Road Scene Parsing,” <b><i>IEEE Transactions on Multimedia</i></b>, doi: 10.1109/TMM.2021.3086618. <span style="color:blue">(SCI, ESI Highly Cited Paper)</span> <a href="https://ieeexplore.ieee.org/document/9447924" target="_blank">[IEEE Xplore] </a> <a href="https://pan.baidu.com/s/1dm1ZKfEStRNCdNabFcj8wQ">[Results]</a> 提取码: 1d45 
<br></li>
 
<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, S. Dong, J. Lei, and L. Yu, “MTANet: Multitask-Aware Network with Hierarchical Multimodal Fusion for RGB-T Urban Scene Understanding,” <b><i>IEEE Transactions on Intelligent Vehicles </i></b>, doi: 10.1109/TIV.2022.3164899. <span style="color:blue">(SCI, ESI Highly Cited Paper)</span> <a href="https://ieeexplore.ieee.org/document/9749834" target="_blank">[IEEE Xplore] </a> 
<br></li>

<li style="margin-bottom: 15px"> <b>S. Dong, W. Zhou*</b>, C. Xu, and W. Yan, "EGFNet: Edge-Aware Guidance Fusion Network for RGB–Thermal Urban Scene Parsing," <b><i>IEEE Transactions on Intelligent Transportation Systems</i></b>, doi: 10.1109/TITS.2023.3306368. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/10234530/" target="_blank">[IEEE Xplore] </a> 
<br></li>

<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, Y. Lv, J. Lei, and L. Yu, "Embedded Control Gate Fusion and Attention Residual Learning for RGB–Thermal Urban Scene Parsing," <b><i>IEEE Transactions on Intelligent Transportation Systems</i></b>, doi: 10.1109/TITS.2023.3242651. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/10041960/" target="_blank">[IEEE Xplore] </a> 
<br></li>

<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, S. Dong, M. Fang, and L. Yu, "CACFNet: Cross-Modal Attention Cascaded Fusion Network for RGB-T Urban Scene Parsing," <b><i>IEEE Transactions on Intelligent Vehicles</i></b>, doi: 10.1109/TIV.2023.3314527. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/10251592/" target="_blank">[IEEE Xplore] </a> 
<br></li>
 
<li style="margin-bottom: 15px"><b>W. Zhou*</b>, T. Gong, J. Lei and L. Yu, “DBCNet: Dynamic Bilateral Cross-Fusion Network for RGB-T Urban Scene Understanding in Intelligent Vehicles,” <b><i>IEEE Transactions on Systems, Man, and Cybernetics: Systems</i></b>,  doi: 10.1109/TSMC.2023.3298921. <span style="color:blue">(SCI, ESI Highly Cited Paper)</span> <a href="https://ieeexplore.ieee.org/document/10217340" target="_blank">[IEEE Xplore] </a>
<br></li>

</div>
</ul>

#### RGB–D Semantic Segmentation

<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">

<li style="margin-bottom: 15px"><b>W. Zhou*</b>, Y. Cai, L. Zhang, W. Yan, and L. Yu, "UTLNet: Uncertainty-aware Transformer Localization Network for RGB-Depth Mirror Segmentation,” <b><i>IEEE Transactions on Multimedia</i></b>, doi: 10.1109/TMM.2023.3323890. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/10278472/" target="_blank">[IEEE Xplore] </a>  
<br></li>

<li style="margin-bottom: 15px"><b>W. Zhou*</b>, Y. Xiao, W. Yan, and L. Yu, "CMPFFNet: Cross-Modal and Progressive Feature Fusion Network for RGB-D Indoor Scene Semantic Segmentation,” <b><i>IEEE Transactions on Automation Science and Engineering</i></b>, doi: 10.1109/TASE.2023.3313122. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/10252155/" target="_blank">[IEEE Xplore] </a>  
<br></li>

<li style="margin-bottom: 15px"><b>W. Zhou*</b>, E. Yang, J. Lei, J. Wan, and L. Yu , "PGDENet: Progressive Guided Fusion and Depth Enhancement Network for RGB-D Indoor Scene Parsing,” <b><i>IEEE Transactions on Multimedia</i></b>, doi: 10.1109/TMM.2022.3161852. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/9740493/" target="_blank">[IEEE Xplore] </a>  
<br></li>
	
<li style="margin-bottom: 15px"><b>W. Zhou*</b>, E. Yang, J. Lei, L. Yu, "FRNet: Feature Reconstruction Network for RGB-D Indoor Scene Parsing,” <b><i>IEEE Journal of Selected Topics in Signal Processing</i></b>, doi: 10.1109/JSTSP.2022.3174338. <span style="color:blue">(SCI, ESI Hot Paper, ESI Highly Cited Paper)</span> <a href="https://ieeexplore.ieee.org/document/9774020" target="_blank">[IEEE Xplore] </a> <a href="https://github.com/EnquanYang2022/FRNet">[Code & Results]</a>
<br></li>

<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, J. Lei, T. Luo, “TSNet: Three-stream Self-attention Network for RGB-D Indoor Semantic Segmentation,” <b><i>IEEE Intelligent Systems</i></b>, vol. 36, no. 4, pp. 73-78, 2021. doi: 10.1109/MIS.2020.2999462. <span style="color:blue">(SCI, Popular Documents, ESI Highly Cited Paper)</span> <a href="https://ieeexplore.ieee.org/document/9113665" target="_blank">[IEEE Xplore] </a> 
<br></li>
	
<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, S. Lv, J. Lei, and L. Yu, "RFNet: Reverse Fusion Network with Attention Mechanism for RGB-D Indoor Scene Understanding," <b><i>IEEE Transactions on Emerging Topics in Computational Intelligence</i></b>, doi: 10.1109/TETCI.2022.2160720. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/9755197/" target="_blank">[IEEE Xplore] </a> 
<br></li>

<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, G. Xu, F. Qiang, and L. Yu, "ACENet: Auxiliary Context-Information Enhancement Network for RGB-D Indoor Scene Semantic Segmentation," <b><i>IEEE Transactions on Emerging Topics in Computational Intelligence</i></b>, doi: 10.1109/TETCI.2023.3303930. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/10226338/" target="_blank">[IEEE Xplore] </a> 
<br></li>	
		
<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, Y. Yue, M. Fang, X. Qian, R. Yang, and L. Yu, "BCINet: Bilateral cross-modal interaction network for indoor scene understanding in RGB-D images," <b><i>Information Fusion </i></b>, doi: 10.1016/j.inffus.2023.01.016. <span style="color:blue">(SCI, ESI Hot Paper, ESI Highly Cited Paper)</span> <a href="https://www.sciencedirect.com/science/article/abs/pii/S1566253523000234/" target="_blank">[Science Direct] </a> 
<br></li>

</div>
</ul>

#### Remote Sensing Images

<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">

<li style="margin-bottom: 15px"><b>W. Zhou*</b>, X. Fan, W. Yan, S. Shan, Q. Jiang, and J.-N. Hwang, "Graph Attention Guidance Network With Knowledge Distillation for Semantic Segmentation of Remote Sensing Images," <b><i>IEEE Transactions on Geoscience and Remote Sensing</i></b>, doi: 10.1109/TGRS.2023.3311480. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/10244111" target="_blank">[IEEE Xplore] </a> <a href="https://github.com/F8AoMn/GAGNet-KD">[Code & Results]</a> 
<br></li>

<li style="margin-bottom: 15px"><b>W. Zhou*</b>, J. Jin, J. Lei, and J.-N. Hwang, "CEGFNet: Common Extraction and Gate Fusion Network for Scene Parsing of Remote Sensing Images," <b><i>IEEE Transactions on Geoscience and Remote Sensing</i></b>, doi: 10.1109/TGRS.2021.3109626. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/9538389" target="_blank">[IEEE Xplore] </a> <a href="https://github.com/NIT-JJH/CEGFNet">[Code & Results]</a> 
<br></li>
	
<li style="margin-bottom: 15px"><b>W. Zhou*</b>, J. Jin, J. Lei, L. Yu, "CIMFNet: Cross-layer Interaction and Multiscale Fusion Network for Semantic Segmentation of High-Resolution Remote Sensing Images,” <b><i>IEEE Journal of Selected Topics in Signal Processing</i></b>, doi: 10.1109/JSTSP.2022.3159032. <span style="color:blue">(SCI, ESI Highly Cited Paper)</span> <a href="https://ieeexplore.ieee.org/document/9735276" target="_blank">[IEEE Xplore] </a> 
<br></li>

<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, X. Fan, L. Yu and J. Lei,"MISNet: Multiscale Cross-layer Interactive and Similarity Refinement Network for Scene Parsing of Aerial Images," <b><i>IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing </i></b>, doi: 10.1109/JSTARS.2023.3243247. <span style="color:blue">(SCI, ESI Hot Paper)</span> <a href="https://ieeexplore.ieee.org/document/10040743/" target="_blank">[IEEE Xplore] </a> 
<br></li>
		
<li style="margin-bottom: 15px"> <b>X. Fan, W. Zhou*</b>, X. Qian, and W. Yan, "Progressive Adjacent-Layer coordination symmetric cascade network for semantic segmentation of Multimodal remote sensing images," <b><i>Expert Systems with Applications </i></b>, doi: 10.1016/j.eswa.2023.121999. <span style="color:blue">(SCI)</span> <a href="https://www.sciencedirect.com/science/article/abs/pii/S0957417423025010/" target="_blank">[Science Direct] </a>
<br></li>

</div>
</ul>

#### Surface Defect Inspection

<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">

<li style="margin-bottom: 15px"> <b>W. Zhou</b>, K. Hong, W. Yan, and Q. Jiang, "Modal Evaluation Network via Knowledge Distillation for No-Service Rail Surface Defect Detection," <b><i> IEEE Transactions on Circuits and Systems for Video Technology</i></b>, doi: 10.1109/TCSVT.2023.3325229. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/10286557/" target="_blank">[IEEE Xplore] </a>  <a href="https://github.com/hjklearn/Rail-Defect-Detection">[Code & Results]</a><br></li>

<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, and K. Hong, "FHENet: Lightweight Feature Hierarchical Exploration Network for Real-Time Rail Surface Defect Inspection in RGB-D Images," <b><i> IEEE Transactions on Instrumentation and Measurement</i></b>, doi: 10.1109/TIM.2023.3237830. <span style="color:blue">(SCI, ESI Hot Paper, ESI Highly Cited Paper)</span> <a href="https://ieeexplore.ieee.org/document/10019291/" target="_blank">[IEEE Xplore] </a>  <a href="https://github.com/Wang-5ying/PENet-KD">[Code & Results]</a> 
<br></li>	

<li style="margin-bottom: 15px"> B. Wang, <b>W. Zhou*</b>, W. Yan, Q. Jiang, and R. Cong, "PENet-KD: Progressive Enhancement Network via Knowledge Distillation for Rail Surface Defect Detection," <b><i> IEEE Transactions on Instrumentation and Measurement</i></b>, doi: 10.1109/TIM.2023.33330222. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/10309228/" target="_blank">[IEEE Xplore] </a>  <a href="https://github.com/hjklearn/Rail-Defect-Detection">[Code & Results]</a> 
<br></li>

</div>
</ul>

#### Crowd Counting

<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">
	
<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, Y. Pan, L. Ye, and L. Yu, "DEFNet: Dual-Branch Enhanced Feature Fusion Network for RGB-T Crowd Counting," <b><i> IEEE Transactions on Intelligent Transportation Systems</i></b>, doi: 10.1109/TITS.2022.3203385. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/9889192/" target="_blank">[IEEE Xplore] </a>  <a href="https://github.com/panyi95/DEFNet">[Code & Results]</a> 
<br></li>	
  
<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, X. Yang, J. Lei, W. Yan, and L. Yu, "MC 3 Net: Multimodality Cross-Guided Compensation Coordination Network for RGB-T Crowd Counting," <b><i> IEEE Transactions on Intelligent Transportation Systems</i></b>, doi: 10.1109/TITS.2023.3321328. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/10285562/" target="_blank">[IEEE Xplore] </a>  <a href="https://github.com/WBangG/MC3Net">[Code & Results]</a> 
<br></li>

</div>
</ul>

#### Image Quality Assessment
  
<ul class="graid3-ul">
<div style="text-align: justify; display: block; margin-right: auto;">
  
<li style="margin-bottom: 15px"><b>W. Zhou*</b>, L. Yu, Y. Zhou, W. Qiu, M. Wu and T. Luo, “Local and Global Feature Learning for Blind Quality Evaluation of Screen Content and Natural Scene Images,” <b><i>IEEE Transactions on Image Processing</i></b>, vol. 27, no. 5, pp. 2086-2095, May 2018, doi: 10.1109/TIP.2018.2794207. <span style="color:blue">(SCI, CCF A类)</span> <a href="https://ieeexplore.ieee.org/document/8259349" target="_blank">[IEEE Xplore] </a> 
<br></li>

<li style="margin-bottom: 15px"><b>W. Zhou*</b> and L. Yu, “Binocular Responses for No-Reference 3D Image Quality Assessment,” <b><i>IEEE Transactions on Multimedia</i></b>, vol. 18, no. 6, pp. 1077-1084, June 2016, doi: 10.1109/TMM.2016.2542580. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/7434031" target="_blank">[IEEE Xplore] </a> 
<br></li>

<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, W. Qiu and M. Wu, “Utilizing Dictionary Learning and Machine Learning for Blind Quality Assessment of 3-D Images,” <b><i>IEEE Transactions on Broadcasting</i></b>, vol. 63, no. 2, pp. 404-415, June 2017, doi: 10.1109/TBC.2016.2638620. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/7811252" target="_blank">[IEEE Xplore] </a> 
<br></li>

<li style="margin-bottom: 15px"> <b>W. Zhou*</b>, J. Lei, Q. Jiang, L. Yu and T. Luo, “Blind Binocular Visual Quality Predictor Using Deep Fusion Network,” <b><i>IEEE Transactions on Computational Imaging</i></b>, vol. 6, pp. 883-893, 2020, doi: 10.1109/TCI.2020.2993640. <span style="color:blue">(SCI)</span> <a href="https://ieeexplore.ieee.org/document/9093188" target="_blank">[IEEE Xplore] </a> 
<br></li>
<li style="margin-bottom: 15px"><b>W. Zhou*</b>, L. Yu, Y. Zhou, W. Qiu, M.-W. Wu, T. Luo, “Blind quality estimator for 3D images based on binocular combination and extreme learning machine,” <b><i>Pattern Recognition</i></b>, vol. 71, pp. 207–217, Nov. 2017. <span style="color:blue">(SCI)</span> 
<br></li>

</div>
</ul>	

<a href="https://clustrmaps.com/site/1bsiu"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=_STtdGdg0IG1p3VGKdLhg5kqLMmiLIfMpCIbz67P0Tc&cl=ffffff" /></a>
